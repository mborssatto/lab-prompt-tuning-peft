{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCA0sm55VaS-",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Lab | Introduction to Prompt Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Fine-tune a Foundational Model effortless -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
          "showTitle": false,
          "title": ""
        },
        "id": "2vkOvTEsVaTA"
      },
      "source": [
        "# Prompt Tuning\n",
        "\n",
        "## Brief introduction to Prompt Tuning.\n",
        "It’s an Additive Fine-Tuning technique for models. This means that we WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL. You might be wondering, how are we going to perform fine-tuning then? Well, we will train additional layers that are added to the model. That’s why it’s called an Additive technique.\n",
        "\n",
        "Considering it’s an Additive technique and its name is Prompt-Tuning, it seems clear that the layers we’re going to add and train are related to the prompt.\n",
        "\n",
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_5_Prompt_Tuning.jpg?raw=true)\n",
        "\n",
        "We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, that particular section of the prompt cannot be translated into natural language. **It's as if we've mastered expressing ourselves in embeddings and generating highly effective prompts.**\n",
        "\n",
        "In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt.\n",
        "\n",
        "The primary consequence of this technique is that the number of parameters to train is genuinely small. However, we encounter a second, perhaps more significant consequence, namely that, **since we do not modify the weights of the pretrained model, it does not alter its behavior or forget any information it has previously learned.**\n",
        "\n",
        "The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered\n",
        "\n",
        "## What are we going to do in the notebook?\n",
        "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One will be trained to generate prompts and the other to detect hate in sentences.\n",
        "\n",
        "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZhdbTh-VaTA"
      },
      "source": [
        "## Loading the Peft Library\n",
        "This library contains the Hugging Face implementation of various fine-tuning techniques, including Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "JechhJhhVaTA"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.10.0\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGbh426RVaTB"
      },
      "source": [
        "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "id": "KWOEt-yOVaTB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYsnwjSVaTC"
      },
      "source": [
        "## Loading the model and the tokenizers.\n",
        "\n",
        "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning.\n",
        "\n",
        "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab. Feel Free to try with a bigger one if you have acces to a good GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MnqIhv2UVaTC"
      },
      "outputs": [],
      "source": [
        "model_name = \"bigscience/bloom-560m\"\n",
        "NUM_VIRTUAL_TOKENS = 4\n",
        "#If you just want to test the solution, you can reduce the EPOCHs.\n",
        "NUM_EPOCHS_PROMPT = 3\n",
        "NUM_EPOCHS_CLASSIFIER = 2\n",
        "device = \"cuda\" #Replace with \"mps\" for Silicon chips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSMu3qRsVaTC"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map = device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2fWhOnVaTC"
      },
      "source": [
        "## Inference with the pre trained bloom model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "47j2D3WWVaTC"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100): #PLAY WITH THIS FUNCTION AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
          "showTitle": false,
          "title": ""
        },
        "id": "kRLSfuo2VaTC"
      },
      "source": [
        "To compare the pre-trained model with the same model after the prompt-tuning process, I will run the same sentence on both models.\n",
        "\n",
        "Since I'm creating a model that can generate prompts, I'll instruct it to provide a prompt that makes it act like a fitness trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvStaT7cVaTC",
        "outputId": "79bacf19-31e0-478c-8a86-41d899c375d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Act like a motivational coach. I have been working with people who are struggling to get their goals and dreams realized, or just want some help getting there.\\nI am an entrepreneurial psychologist trained in the field of human development by The University Of California at San Diego (U']\n"
          ]
        }
      ],
      "source": [
        "input_prompt = tokenizer(\"Act like a motivational coach\", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_prompt.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-PY1yAh2mB"
      },
      "source": [
        "The model doesn't know what its mission is and answers as best as it can. It's not a bad response, but it's not what we're looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
          "showTitle": false,
          "title": ""
        },
        "id": "OGbJTbRnVaTD"
      },
      "source": [
        "# Prompt Creator\n",
        "## Preparing Datasets\n",
        "The Dataset used, for this first example, is:\n",
        "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RD8H_LLaVaTD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
          "showTitle": false,
          "title": ""
        },
        "id": "xmAp_o4PVaTD"
      },
      "outputs": [],
      "source": [
        "dataset_prompt = \"fka/awesome-chatgpt-prompts\"# - You may use the one we used in the lesson if can't find a decent one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qaU4FmgwAzZK"
      },
      "outputs": [],
      "source": [
        "def concatenate_columns_prompt(dataset):\n",
        "    def concatenate(example):\n",
        "        example['prompt'] = \"Act as a {}. Prompt: {}\".format(example['act'], example['prompt'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ee60f333952248b999b01d2911acecc2",
            "a92f78f2ddb947ffb696091d8bc0f170",
            "8994a85e591843d38e81b280e8eb8e90",
            "7529540ecb8a4fa38043ad85b552b940",
            "32309359963440ccbb61773008d9891b",
            "c428b0110e834cd888ae813da0ce087e",
            "a3756cea102b4ea8b6fe894e7f32d254",
            "06f061afad404176bfcd44cc306675c2",
            "3a5a8dd5bdff4fd1bf14e4bbd0560dad",
            "43ad73eff10d4ae18e19490ddffb55a7",
            "c46d7144426f4b0e9d9c955e88960660"
          ]
        },
        "id": "uoL6qitsLo0o",
        "outputId": "7018a359-9d89-4c81-b659-3f7d86c2dc14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee60f333952248b999b01d2911acecc2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Create the Dataset to create prompts.\n",
        "data_prompt = load_dataset(dataset_prompt)\n",
        "data_prompt['train'] = concatenate_columns_prompt(data_prompt['train'])\n",
        "\n",
        "# Tokenization with correct parameters\n",
        "data_prompt = data_prompt.map(\n",
        "    lambda samples: tokenizer(\n",
        "        samples[\"prompt\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=None  # Don't return tensors at this stage\n",
        "    ),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Remove all text columns, keep only tokenized data\n",
        "train_sample_prompt = data_prompt[\"train\"].remove_columns(['act', 'prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL1CwC9tGGSn",
        "outputId": "60f776a4-09e2-4322-e4da-5e1e204b25e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 203\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzA3rLTk8XW8",
        "outputId": "d28ca9f2-eabd-4916-ba23-51f34f39253b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 174249, 36949, 1309, 17, 36949, 1309, 29, 43252, 15202, 51, 46712, 15, 7932, 660, 67606, 613, 660, 8723, 861, 2152, 722, 415, 15, 1874, 17848, 664, 368, 70062, 38038, 388, 174249, 39841, 9427, 11173, 664, 368, 7921, 1581, 9649, 1485, 8943, 17, 137151, 6216, 24466, 87480, 6399, 17, 109988, 368, 70062, 32993, 461, 368, 10082, 3164, 6426, 17, 5070, 5546, 13773, 461, 368, 67606, 15, 13756, 368, 14679, 11210, 17, 137151, 209147, 86, 13773, 361, 368, 67606, 10136, 15, 11173, 664, 6199, 3466, 9283, 13773, 1485, 8943, 613, 368, 70062, 17, 3904, 67606, 6591, 722, 5636, 53180, 530, 65604, 15, 1427, 861, 473, 1400, 7932, 267, 415, 15, 1874, 14679, 8723, 1485, 718, 17, 143293, 267, 3829, 4737, 461, 499, 49863, 530, 557, 103096, 135158, 17251, 427, 2670, 70062, 17, 30497, 13756, 2914, 3390, 17848, 17251, 427, 368, 70062, 17, 121045, 1074, 267, 4737, 461, 735, 24466, 26331, 31437, 427, 13756, 530, 368, 49635, 91770, 5484, 17, 42187, 11097, 3291, 87099, 1130, 174169, 14476, 17, 152830, 368, 67606, 3727, 1571, 404, 530, 1571, 415, 17]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_prompt[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
          "showTitle": false,
          "title": ""
        },
        "id": "0-5mv1ZpVaTD"
      },
      "source": [
        "## prompt-tuning configuration.  \n",
        "\n",
        "API docs:\n",
        "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
          "showTitle": false,
          "title": ""
        },
        "id": "sOg1Yh-oVaTD"
      },
      "outputs": [],
      "source": [
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "generation_config_prompt = PromptTuningConfig( #PLAY WITH THIS CONFIG IF YOU LIKE\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9KBtB1VaTD"
      },
      "source": [
        "We will create two  prompt tuning models using the same pre-trained model and the same config, but with a different Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_D8oDQZVaTD",
        "outputId": "5ce68ceb-122d-48f4-e57b-c64c90c1979c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model_prompt = get_peft_model(foundational_model, generation_config_prompt)\n",
        "print(peft_model_prompt.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
          "showTitle": false,
          "title": ""
        },
        "id": "i6WhJSUwVaTE"
      },
      "source": [
        "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
        "\n",
        "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SJoznfzjVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        #use_cpu=True, # This is necessary for CPU clusters.\n",
        "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
        "        #per_device_train_batch_size=4,\n",
        "        num_train_epochs=epochs\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
          "showTitle": false,
          "title": ""
        },
        "id": "cb1j50DSVaTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "working_dir = \"./\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
        "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)\n",
        "if not os.path.exists(output_directory_prompt):\n",
        "    os.mkdir(output_directory_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5IhO9mVaTE"
      },
      "source": [
        "We need to indicate the directory containing the model when creating the TrainingArguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
          "showTitle": false,
          "title": ""
        },
        "id": "GdMfjk5RVaTE"
      },
      "source": [
        "## Training first model\n",
        "\n",
        "We will create the trainer Object, one for each model to train.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "D4v4RSSeVaTE"
      },
      "outputs": [],
      "source": [
        "training_args_prompt = create_training_arguments(output_directory_prompt,\n",
        "                                                 3e-2,\n",
        "                                                 NUM_EPOCHS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uVAfNdEIVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling, TrainerCallback\n",
        "\"\"\"def create_trainer(model, training_args, train_dataset):\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to train the model.\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        "    )\n",
        "    return trainer\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Custom callback for memory cleanup\n",
        "class MemoryCleanupCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if torch.backends.mps.is_available():\n",
        "            torch.mps.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    # Create data collator with correct settings\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # mlm=False indicates not to use masked language modeling\n",
        "        pad_to_multiple_of=8,  # Alignment for efficiency\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to train the model.\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[MemoryCleanupCallback()] # Add callback for memory cleanup\n",
        "    )\n",
        "    return trainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "1Sz9BeFZVaTF",
        "outputId": "44962e2a-d296-4765-a822-ebb64aa2d366"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 02:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=78, training_loss=3.1067653558193107, metrics={'train_runtime': 147.8651, 'train_samples_per_second': 4.119, 'train_steps_per_second': 0.528, 'total_flos': 486047507742720.0, 'train_loss': 3.1067653558193107, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "#Training first model.\n",
        "trainer_prompt = create_trainer(peft_model_prompt,\n",
        "                                training_args_prompt,\n",
        "                                train_sample_prompt)\n",
        "trainer_prompt.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veg8ziHvWh4I"
      },
      "source": [
        "Release GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYVcJDSP7Sq-",
        "outputId": "b0f51cec-85a8-4cfc-fc29-59a87328d3a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "287"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
          "showTitle": false,
          "title": ""
        },
        "id": "s5k10HwoVaTG"
      },
      "source": [
        "## Save model\n",
        "We are going to save the model. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
          "showTitle": false,
          "title": ""
        },
        "id": "E3dn3PeMVaTG"
      },
      "outputs": [],
      "source": [
        "trainer_prompt.model.save_pretrained(output_directory_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
          "showTitle": false,
          "title": ""
        },
        "id": "rkUKpDDWVaTG"
      },
      "source": [
        "## Inference first tuned model\n",
        "\n",
        "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
          "showTitle": false,
          "title": ""
        },
        "id": "dlqXXN8oVaTG"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
        "                                         output_directory_prompt,\n",
        "                                         #device_map=device,\n",
        "                                         is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "jjXT-6EKMiXk",
        "outputId": "69030630-7570-4020-9e93-9982b9bc55b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3225992843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                           \u001b[0minput_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           max_new_tokens=50)\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_prompt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3277119473.py\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(model, inputs, max_new_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this function returns the outputs from the model received, and inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#PLAY WITH THIS FUNCTION AS YOU SEE FIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mprepare_inputs_for_generation\u001b[0;34m(self, task_ids, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0;31m# In prompt learning methods, past key values are longer when compared to the `input_ids`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 \u001b[0;31m# As such only consider the last input ids in the autogressive generation phase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n",
        "                                          input_prompt,\n",
        "                                          max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzjDgDE2reTO"
      },
      "source": [
        "Let's compare the result of the model before and after being fine-tuned with prompt-tuning.\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:  Follow up with your trainer\n",
        "```\n",
        "**Trained for classification with Prompt-tuning** 50 Epochs:\n",
        "```\n",
        "Act as a fitness Trainer. Prompt: ＋ Acts like an expert in the field of sports and health, but does not provide detailed information about his work or products to help you understand them better.  + I want my first client referred me through this website for their gym membership program which is based on physical activity training exercises that are easy enough (eight minutes) per week with no need any special equipment required.   - First Question : What would be your role?\n",
        "```\n",
        "\n",
        "It's very clear that the result is quite different, it's not exactly what we're looking for but it's much closer.\n",
        "\n",
        "It's possible that we're at the limit of what Bloom's smallest model can offer. Try with any other model, surely with the one with 1B parameters the result will be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVif-42UCP7l"
      },
      "source": [
        "# Hate Classifier\n",
        "##Loading the Dataset\n",
        "\n",
        "* https://huggingface.co/datasets/SetFit/ethos_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyp64F0tRQBt",
        "outputId": "19d8c50f-06a7-48e1-cbdc-2165c6c56e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sentence : Head is the shape of a light bulb. Label :  head']\n"
          ]
        }
      ],
      "source": [
        "input_classifier = tokenizer(\"YOUR SENTENCE HERE \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_classifier.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gk7C_NSt-Y"
      },
      "source": [
        "The model has no idea what its purpose is, so it completes the sentence as best as it can."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlLAsUvhE09L"
      },
      "outputs": [],
      "source": [
        "dataset_classifier = \"YOUR DATASET HERE\"\n",
        "\n",
        "def concatenate_columns_classifier(dataset):\n",
        "    def concatenate(example):\n",
        "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "7fbfcba0fbf845df97135421d11cff8b",
            "ea853488e0a645eb8168af2a1ff5d1e8",
            "b5858ecf0fd54aec84ae1e7be9b8fc60",
            "4d35981fa77d4dcd8fae5049ab08b620",
            "776857b40f2f43688849e9acf95dae90",
            "9b667e71de094898bcaaf6e68ba2e293",
            "384cc41a597b4fc796024556899da6b3",
            "e65c0086cc864449af8a3302613de89f",
            "3c70b669daa943e7a21049e20cc3135b",
            "2778dc9518b24de58fb72fed5367b65b",
            "d2b9b9764b3743018e12f35a426919d4",
            "4268f0284ebb4092828a26323dc2f0a7",
            "b9a18cecada54455956241a9ade2824d",
            "16a3ff4c22e54261b7ce950f613d539a",
            "bd52e2af0c07454da7c6056f4ad1d007",
            "2d5d6b103d6d4892b52f18cacc71dd59",
            "cdc26d18b0b94f2b8661843dfd6bc218",
            "f6136f01bb0743b4b6621a21fb44477b",
            "06a558f9da7b49c997f49273d8736d46",
            "8dbd818be7f64bf9b59d8e397055cf5a",
            "81dc00c886184b45bbe4ef760229d8fc",
            "b5258707caa14a8a909ce99d424506c1",
            "95464341678f4d6e842f29270ce2aa85",
            "2878ac4a3da340b2b9350d9eb5b65608",
            "f4bbf71415b149f6bb88ac85f796f65d",
            "0bda2364da1e4da692430f29fd4edb2b",
            "3c1a13aedf1a45608ff170177446181b",
            "e33801b4e754489196f90c8521b7bf72",
            "0583aea4a2d149388b7a1dd1a82da180",
            "13ad17a8f6ec4b04861b6ca3592fbd7f",
            "0bed840b1eb145c1a93d0207c43cf0e7",
            "3f5008fba05e4f519d39bba6b4a0c1d7",
            "3738552e82334889a33c83afa148ea07",
            "783e0c10b9464e25bbfe5aa141def3cf",
            "495e354126934356a813477b9d8def7d",
            "3e310575ae5548efad3bb8449d08acad",
            "83bcc01874924379adb5e091b90d0a1a",
            "6418520fac804032a9074219a09fc474",
            "104e9826d24f4d2b9a021b0f83a077ca",
            "1b589c05f99b46edbaf8a14e0aa0484a",
            "62fe3966df094bf4a56447bc4e73b0f0",
            "58f3eb69bc7c4a26bf592e51c04dbb2e",
            "49b5fa9719454a79b39ef60e2b451e75",
            "f4b3ee0f7f9f4d2e9f272b41437369c0",
            "ef1815262fe14b05b802d10165dba688",
            "1cc00ddb3e344c63944d772010be41f0",
            "347f825b7a5340e0b63aefb77b56c041",
            "74dbe0adcc094532ba1d4d043db115e5",
            "9fef0d945f0141a287d7cab93a9eb84a",
            "6ef062e3e24f4c5f9a0be9650159fd53",
            "5ca790e4826a4c769513bdfc888afd4b",
            "2338b89c6e204a3cac5333a94de186f2",
            "d5756923c6024b749d2ad0c09b20844d",
            "e5b33be7d58143c592ac602a4bfb9508",
            "ce827d0df4964e8d84d78d1b27b90076",
            "61e3398530f04b65bd47512ee92393f4",
            "8eef47dd0daa40e1a536ecfd77aa9264",
            "039788156e0942ce8dc75ef237316fdd",
            "1f3d7a2c44ee4f989a55d574e63af895",
            "95f2afd0c66e441abdeaff064c7d9ed8",
            "62f9e10f187a45e896b666c684e24fe3",
            "fefa7a6b8fbb453fa92170f38105eebd",
            "297cdbf0eeaa45ec95b189fc4746d284",
            "d0e40ab9d65a4ddbbe516d59a88ef169",
            "b94eea626f51430cba14713fdbe5ad01",
            "cea9738442c445928346e3d7889a98c1",
            "b4767165ba314788b0fc5c720db06827",
            "8950df7182484cf6a773d483f542f2bc",
            "b76449f699c442f2991aa43394a91082",
            "37fabc9965bf45749830f19f3bccfc44",
            "2b24d81984bc4c598d7cfde0f6db8a24",
            "010289abcdc84f74bdfdcd98f2a892ef",
            "3cf300b66f0a43389aa769d800ed4554",
            "ebccac13d3214d76b00c6f64ab11dc2e",
            "71e68629207945d4bb853063e810b3fa",
            "fef798d9e2024cab9246093cb03692d4",
            "c69a6f3f933a48c88dde714bc0801d03",
            "bc26bf19f381497288425556a4fb34a4",
            "0251790f26bb4a26a3b5486a7bdb5ff5",
            "7e3fedb5940f40db92f54335b8e238be",
            "03b654221afb40288b846dc886294bef",
            "6c1d5afe0559496d9e51b6965ca86f0b",
            "031fab9201374fbda6d0414f3deb814e",
            "b903c360b88b4065aa623f8b092e126a",
            "22987cced8064410998b2b8e36f35d15",
            "792766e37ab548b28fbb219517bc10ba",
            "631d486ffc974bd4b9eae7a88e8fab81",
            "c31094667382417eaf1910564b35ada5"
          ]
        },
        "id": "5Y2K_3MqE4QV",
        "outputId": "14a85717-84a6-4252-81e1-533fee46606b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fbfcba0fbf845df97135421d11cff8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/162 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4268f0284ebb4092828a26323dc2f0a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95464341678f4d6e842f29270ce2aa85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/64.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "783e0c10b9464e25bbfe5aa141def3cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef1815262fe14b05b802d10165dba688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61e3398530f04b65bd47512ee92393f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/598 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4767165ba314788b0fc5c720db06827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/598 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc26bf19f381497288425556a4fb34a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_classifier = load_dataset(dataset_classifier)\n",
        "data_classifier['train'] = concatenate_columns_classifier(data_classifier['train'])\n",
        "\n",
        "data_classifier = data_classifier.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
        "train_sample_classifier = data_classifier[\"train\"].remove_columns(['label', 'label_text', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia-vz3ddTOY5",
        "outputId": "4697abdb-6a01-484b-8256-c4610b601f18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 598\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 400\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ROxvTEFAVC",
        "outputId": "cf2331a5-3457-4194-a46b-54b81408a6e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 598\n",
              "})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sample_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2Z_LiRTMDC"
      },
      "source": [
        "I have deleted all the columns from the dataset that are not strictly necessary for training, that is to say, I have removed all columns that are not essential for the model's learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUZNrAdFDR5",
        "outputId": "9a69ac6d-91aa-4105-afb5-13dda525007f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[62121, 1671, 915, 473, 760, 10190, 513, 16154, 60, 19821, 138929, 20812, 426, 18833, 18816, 75536, 45617, 39469, 19368, 17956, 57274, 3758, 18065, 38, 44140, 17956, 72870, 8309, 9492, 15, 614, 156801, 85061, 48283, 44419, 426, 16472, 96789, 602, 45227, 43111, 181485, 435, 19821, 60, 48283, 44419, 426, 16472, 96789, 614, 156801, 77658, 915, 74549, 40423]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_classifier[1:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_F3oR70FF4z"
      },
      "source": [
        "## prompt-tuning configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr0Aw_byFL6n"
      },
      "outputs": [],
      "source": [
        "generation_config_classifier = PromptTuningConfig( #PLAY WITH THIS AS YOU SEE FIT\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,  #\n",
        "    prompt_tuning_init_text=\"Indicates whether the sentence contains hate speech or not\",\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWvoJMajFPcg",
        "outputId": "a6d0bc1e-962a-4718-b66c-30da13edd9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,192 || all params: 559,222,784 || trainable%: 0.0014648902430985358\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model_classifier = get_peft_model(foundational_model, generation_config_classifier)\n",
        "print(peft_model_classifier.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpKtEudsFWTq"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_directory_classifier):\n",
        "    os.mkdir(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DknsYEGwFW4g"
      },
      "outputs": [],
      "source": [
        "training_args_classifier = create_training_arguments(output_directory_classifier,\n",
        "                                                    3e-2,\n",
        "                                                    NUM_EPOCHS_CLASSIFIER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgEAjAMFasw"
      },
      "source": [
        "## Training Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HfDLVy22FaNs",
        "outputId": "b45f5ed2-8b39-41d6-8c8e-466886c996bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='61' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  61/3750 00:16 < 17:39, 3.48 it/s, Epoch 0.80/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='122' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 122/7500 00:14 < 14:47, 8.31 it/s, Epoch 0.81/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14950' max='14950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14950/14950 18:07, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.301100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.287500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.259900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.267900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.242900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>3.245800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>3.222600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>3.219500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>3.206200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.242700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>3.207600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>3.201300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>3.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>3.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>3.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>3.195100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>3.178500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>3.204100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>3.151600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>3.183000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>3.180700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>3.174600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>3.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>3.178500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>3.144500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>3.173000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>3.175700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14950, training_loss=3.2118868069345736, metrics={'train_runtime': 1087.99, 'train_samples_per_second': 27.482, 'train_steps_per_second': 13.741, 'total_flos': 2840548266196992.0, 'train_loss': 3.2118868069345736, 'epoch': 50.0})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_classifier = create_trainer(peft_model_classifier,\n",
        "                                   training_args_classifier,\n",
        "                                   train_sample_classifier)\n",
        "trainer_classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcWL4-4OKCx9"
      },
      "outputs": [],
      "source": [
        "trainer_classifier.model.save_pretrained(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRcSaXMmM3mz"
      },
      "source": [
        "## Inference second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pkx0npHNWja"
      },
      "outputs": [],
      "source": [
        "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "loaded_model_peft.set_adapter(\"classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an8T4LvPJAUC",
        "outputId": "4154df21-1629-4d39-9ff2-9c35e0b839da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sentence : Head is the shape of a light bulb. Label :  no hate speech']\n"
          ]
        }
      ],
      "source": [
        "loaded_model_sentences_outputs = get_outputs(loaded_model_peft,\n",
        "                                             input_classifier, max_new_tokens=3)\n",
        "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbeFTXjVaTG"
      },
      "source": [
        "Let's check how the model's response has changed with training:\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  head\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  No\n",
        "```\n",
        "**Trained for classification with Prompt-tuning**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  no hate speech\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  hate speech\n",
        "```\n",
        "\n",
        "It's clear that the training has fulfilled its purpose. The original model doesn't know what its mission is and tries to complete the sentences as best as it can. On the other hand, the updated model with prompt-tuning does know what its mission is and is able to classify the sentences correctly and in the indicated format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TUjNtGVaTH"
      },
      "source": [
        "# Exercise\n",
        "- Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee60f333952248b999b01d2911acecc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a92f78f2ddb947ffb696091d8bc0f170",
              "IPY_MODEL_8994a85e591843d38e81b280e8eb8e90",
              "IPY_MODEL_7529540ecb8a4fa38043ad85b552b940"
            ],
            "layout": "IPY_MODEL_32309359963440ccbb61773008d9891b"
          }
        },
        "a92f78f2ddb947ffb696091d8bc0f170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c428b0110e834cd888ae813da0ce087e",
            "placeholder": "​",
            "style": "IPY_MODEL_a3756cea102b4ea8b6fe894e7f32d254",
            "value": "Map: 100%"
          }
        },
        "8994a85e591843d38e81b280e8eb8e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06f061afad404176bfcd44cc306675c2",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a5a8dd5bdff4fd1bf14e4bbd0560dad",
            "value": 203
          }
        },
        "7529540ecb8a4fa38043ad85b552b940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ad73eff10d4ae18e19490ddffb55a7",
            "placeholder": "​",
            "style": "IPY_MODEL_c46d7144426f4b0e9d9c955e88960660",
            "value": " 203/203 [00:00&lt;00:00, 3411.48 examples/s]"
          }
        },
        "32309359963440ccbb61773008d9891b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c428b0110e834cd888ae813da0ce087e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3756cea102b4ea8b6fe894e7f32d254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06f061afad404176bfcd44cc306675c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a5a8dd5bdff4fd1bf14e4bbd0560dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43ad73eff10d4ae18e19490ddffb55a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46d7144426f4b0e9d9c955e88960660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}